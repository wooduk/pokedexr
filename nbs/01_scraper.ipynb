{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "import os\n",
    "from io import BytesIO\n",
    "import re\n",
    "import pickle\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "from tqdm import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import boto3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bulbapedia Scraper\n",
    "> A collection of functions to help scrape Pokemon Card content from the Bulbapedia Site "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bulbapedia is an encyclopedia about Pokémon to which anyone can contribute and a useful source for building a labelled dataset of trading card images. \n",
    "\n",
    "For (The Pokedexr Project)[link] the bulbapedia has images of the Japanese cards and the card descriptions in English. \n",
    "\n",
    "There are 3 basic parts here:\n",
    "\n",
    " 1. Some general convenience functions\n",
    "\n",
    " 1. Tools for extracting lists of cards from the [Deck lists](https://bulbapedia.bulbagarden.net/wiki/GX_Starter_Decks_(TCG)) or Set Lists, including a link to the [detailed description page](https://bulbapedia.bulbagarden.net/wiki/Heatmor_(TCG))) for each card.\n",
    " \n",
    " 1. Tools for extracting meta-data, including links to card images, from the detailed description pages.\n",
    " \n",
    " 1. Tools for fetching the card images themselves and storing them.\n",
    " \n",
    "For most of the extraction rules I used the browser's developer tools to find any tags and attributes that would allow me to scrape a list of cards from the deck list pages. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def bulba_url_for(resource): \n",
    "    \"\"\"Returns full url for bulbapedia page given last part or url (resource)\"\"\"\n",
    "    return f\"https://bulbapedia.bulbagarden.net/wiki/{resource}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def sanitize_name(name):\n",
    "    \"\"\"For extracting images we need a cleaned up version of the card name\n",
    "    that appears in Deck of set lists\n",
    "    \"\"\"\n",
    "    return re.sub(re.compile('|'.join(['[ &.-:]',\"'s\"])), '', name)#.replace(\"'s\",'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def fetch_page_soup(resource): \n",
    "    \"\"\"Fetches a page and returns a beautiful soup.\"\"\"\n",
    "    r = requests.get(bulba_url_for(resource))\n",
    "    if r.ok:\n",
    "        soup = BeautifulSoup(r.content, 'html.parser')\n",
    "    else:\n",
    "        soup = None\n",
    "        logging.error(f\"Couldn't fetch resource {bulba_url_for(resource)}.\")\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def save_card_list(cards, fname='cards.pkl'):\n",
    "    \"\"\"Pickles the card list\"\"\"\n",
    "    with open(fname,'wb') as f:\n",
    "        pickle.dump(cards,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export       \n",
    "def load_card_list(cards, fname='cards.pkl'):\n",
    "    \"\"\"Load a pickled card list\"\"\"\n",
    "    with open(fname,'rb') as f:\n",
    "        d = pickle.load(f)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "BUCKET_NAME=os.environ.get('POKE_S3_BUCKETNAME')\n",
    "LOCAL_FOLDER=Path('.')\n",
    "\n",
    "def store_image(image, fname, location): \n",
    "    if location[:2]=='s3':\n",
    "        bucket=location[3:].split('/')[0]\n",
    "        f=location[3:].split('/')[1:]\n",
    "        push_image_to_s3(image, fname, bucket_name=bucket, dr='/'.join(f))\n",
    "    else:\n",
    "        save_image_local(image, fname, dr=location)\n",
    "        \n",
    "def push_image_to_s3(image_data, fname, dr='',bucket_name=''):\n",
    "    s3 = boto3.resource('s3')\n",
    "    key = '/'.join(filter(lambda item: item!='', [dr, fname]))\n",
    "    ret = s3.Bucket(bucket_name).put_object(Key=key, Body=BytesIO(image_data))\n",
    "    if not ret:\n",
    "        logging.warning(f\"Failed to upload {fname} to S3/{bucket_name}.\")\n",
    "    return ret\n",
    "\n",
    "def save_image_local(image_data, fname, dr=''):\n",
    "    Path(dr).mkdir(parents=True, exist_ok=True)\n",
    "    n = '/'.join(filter(lambda item: item != '', [dr, fname]))\n",
    "    with open(n,'wb') as f:\n",
    "        f.write(image_data)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extracting cards from Deck or Set Lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function to fetch a list of card name, numbers and links to details page from bulbapedia.\n",
    "Requires a valid Deck or Set List reference to work. Here are the assumptions about how the page is structured:\n",
    "\n",
    "* For `set lists` the links to the details pages were contained in the `href` attribute of elements that has the class `mw-redirect`.\n",
    "* `Deck Lists` are buried in nested tables. The best I could come up with was to note the headings of the tables are wrapped in \\<b> tags, find those. Then work back up the parents until you find the correct table parent. Now iterate over the rows of that table to extract information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export       \n",
    "def fetch_card_list(list_name, req_decks=[]):\n",
    "    \"\"\"Returns a list of card names, numbers and links to details pages from given list\n",
    "    \n",
    "    list_name -- reference to the page containing the list on bulbapedia \n",
    "    decks -- decks to be taken from a desk list, ignored for set list\n",
    "    \"\"\"\n",
    "    cards={}\n",
    "    soup = fetch_page_soup(list_name)\n",
    "    \n",
    "    # is this a Deck List or Set List?\n",
    "    list_tables = {lt:soup.find(id=lt) for lt in ['Deck_lists', 'Set_list']}\n",
    "    \n",
    "    if list_tables['Deck_lists'] is not None: \n",
    "        # retrieve the table right after the 'Deck Lists' heading\n",
    "        deck_lists_tables = soup.find_all('table',{\"class\": \"multicol\"})\n",
    "\n",
    "        # Pull out the required deck lists\n",
    "        for t in deck_lists_tables:\n",
    "            for deck in req_decks:\n",
    "                e=t.find_all('b',text=deck)\n",
    "                if len(e) > 0:\n",
    "                    _deck = e[0].parent.parent.parent.parent.parent.parent.parent\n",
    "\n",
    "                    for row in _deck.find_all('tr')[4:]:\n",
    "                        try:\n",
    "                            number, name, _ = row.find_all('td')\n",
    "                            durl = name.a.attrs.get('href').split('/')[-1]\n",
    "                            card = {'number':number.text.strip(), 'name': name.a.text, 'details_page_ref': durl}\n",
    "                            cards[card['name']] = card\n",
    "                        except:\n",
    "                            pass\n",
    "    elif list_tables['Set_list'] is not None: \n",
    "        set_list_table = list_tables['Set_list'].findNext('table')\n",
    "        for row in set_list_table.find_all('a',{'class':'mw-redirect'}):\n",
    "            if row:\n",
    "                card = {\n",
    "                    'number':row.parent.parent.find('td').text.strip(),\n",
    "                    'name': row.text,\n",
    "                    'details_page_ref': row.attrs.get('href').split('/')[-1]\n",
    "                }\n",
    "                cards[card['name']]=card\n",
    "    else:\n",
    "        logging.error(f'Could not fetch deck or set list {list_name}')\n",
    "        \n",
    "    # add santized names\n",
    "    for card_name,card in cards.items():\n",
    "        card.update({'sname':sanitize_name(card_name)})\n",
    "    \n",
    "    return cards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://bulbapedia.bulbagarden.net/wiki/Charmander_(GX_Starter_Deck_11)'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#r = requests.get(bulba_url_for(cards['Charmander'].get('details_page_ref','notexist')))\n",
    "#soup = BeautifulSoup(r.content,'html.parser')\n",
    "bulba_url_for(cards['Charmander'].get('details_page_ref','notexist'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content we want is in a nested table structure under (well, after) the heading 'Card Text'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://bulbapedia.bulbagarden.net/wiki/Charmander_(GX_Starter_Deck_11)'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pokemon_card_text_table = soup.find(id='Card_text').findNext()\n",
    "url_for(cards['Charmander'].get('details_page_ref','notexist'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Updating a list of cards with information from the details pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also want some other information from the details page, namely the card text and the numbers of the cards.\n",
    "\n",
    "Extracting the card text was a bit interesting as the format of the html was different for the Trainer cards than Pokemon Cards. (Were not interested in the energy cards here)\n",
    "\n",
    "First an example of the Pokemon card. Let's stick with Charmander."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Image Urls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TODO] Description of Extract the image...\n",
    "\n",
    "The card page can, but doesn't always, include a 'gallery' section where images are displayed if more than one version of the artwork exists. For example different editions of the cards use different artists. At first I'm not sure of what to do with this. On the one hand I know that in my specific case I only have 1 version of the artwork to recognise. But as a human I would still be able to recognise a pokemon across the different depictions. I think it is best to fetch the images and then experiment later with the effect of including/excluding them later.\n",
    "\n",
    "Some of the pages have a gallery section with multiple artworks. The gallery section, if it exists, looks like this:\n",
    "\n",
    "```<span id=\"Gallery> ... <span>```\n",
    "\n",
    "But the table of images that we want is not encapsulated in a `<div>` so we can't use that section heading to get the images embedded as children within it. \n",
    "\n",
    "Instead we can extract all `<img>` tags from the page and then filter out unwanted images. Using developer tools we can see that the pokemon/entity name is embedded in the `alt` attribute of the `img` tag for the images we want. So we can use the name we collected earlier to extract the images we want.\n",
    "\n",
    "A slight complication is that the name as it appears in the attribute differs from the name we've recorded. It has any underscore, dash, spaces, punctuation removed. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De-duplicating the list of image urls\n",
    "\n",
    "The list of image urls extracted often has inexact duplicates: they are just different sizes of the same image. The `dedupe_image_urls` function aims to keep only one set of each image be examining the path but ignoring the image size part of the file name. Implemented using a dictionary so we keep the last version a file that is in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def dedupe_image_urls(urls):\n",
    "    \"\"\"Try to de-duplicate a list of image urls given the path structure\"\"\"\n",
    "    deduped_img_urls = {}\n",
    "    for u in urls:\n",
    "        k = '/'.join(u.split('/')[:-1])\n",
    "        deduped_img_urls[k] = u.split('/')[-1]\n",
    "\n",
    "    return [k+'/'+deduped_img_urls[k] for k in deduped_img_urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now do it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "IMG_EXCEPTIONS = {'Devoured_Field_(GX_Starter_Deck_128)':'Decaying_Wasteland_()'}\n",
    "    \n",
    "def extract_image_urls(soup, name):\n",
    "    \"\"\"Given the details page soup, extract image urls\"\"\"\n",
    "    card_images = soup.findAll('img',{'alt':re.compile(name)})\n",
    "    return dedupe_image_urls(list(set([i.get('src') for i in card_images])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Card Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'Card Text' is the english translation of the main text on the card (ie.. moves, instructions). I couldn't find any useful tags to extract this data so I've just had to create a loop to parse the tables relying on an assumption about the order of the content. It was just trial and error to develop this.\n",
    "\n",
    "The 'Trainer' cards and 'Pokemon' cards have different structures so each has a different function to extract information. Energy cards have no text. Here is a function that determines the difference from the soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export \n",
    "def isTrainerCard(soup): \n",
    "    \"\"\"Given the soup of card details page, detect if this is a trainer card.\"\"\"\n",
    "    s = soup.find(href='/wiki/Trainer_card_(TCG)')\n",
    "    if s is None:\n",
    "        return False\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a function to extract the card text from a Pokemon card. [Charmander](https://bulbapedia.bulbagarden.net/wiki/Charmander_(GX_Starter_Deck_11) is an example of a pokemon card."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def extract_pokemon_card_text(card_text_table):\n",
    "    \"\"\"Given the soup of the details page, return the card text for a trainer card.\"\"\"\n",
    "    items=[]\n",
    "    current_title=None\n",
    "    \n",
    "    japanese_chars = [chr(c) for c in range(0x3040,0x30ff)]\n",
    "    japanese_chars_re ='['+''.join(japanese_chars)+']'\n",
    "\n",
    "    for i,table in enumerate(card_text_table.find_all('table')):\n",
    "        table_text=table.text.strip().replace('\\n','') \n",
    "        \n",
    "        if re.search(japanese_chars_re,table_text):\n",
    "            \n",
    "            energy_items = [i.attrs.get('alt') for i in table.find_all('img')]\n",
    "        \n",
    "            # we are looking at a name item because of the japanese characters\n",
    "            tokens = table_text.split(' ')\n",
    "            try:\n",
    "                points=int(tokens[-1])\n",
    "                name={'jp':tokens[-2], 'en':' '.join(tokens[:-2])}\n",
    "            except:\n",
    "                # Some moves have no points associated\n",
    "                points=None\n",
    "                name={'jp':tokens[-1], 'en':' '.join(tokens[:-1])}\n",
    "\n",
    "            current_title = {\n",
    "                'name': name,\n",
    "                'points':points\n",
    "            }\n",
    "        else:\n",
    "                desc=table_text if len(table_text) else None\n",
    "                # a description for current move\n",
    "                current_item={}\n",
    "                if current_title:\n",
    "                    current_item['type']='move'\n",
    "                    current_item['description']=desc\n",
    "                    current_item['energy_items']=energy_items\n",
    "                    current_item.update(current_title)\n",
    "                elif len(table_text)>0:\n",
    "                    current_item['type']='info'\n",
    "                    current_item['description']=desc\n",
    "                    \n",
    "\n",
    "\n",
    "                if current_item.get('type',False):\n",
    "                    items.append(current_item)\n",
    "\n",
    "                current_title = None\n",
    "    \n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to use it\n",
    "#pokemon_card_text_table = soup.find(id='Card_text').findNext()\n",
    "#extract_pokemon_card_text(pokemon_card_text_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the equivalent for a Trainer card. Trainer cards are slightly different and thankfully, simpler. [Professor Kukui](https://bulbapedia.bulbagarden.net/wiki/Professor_Kukui_(GX_Starter_Deck_118)) is an example trainer card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def extract_trainer_card_text(card_text_table):\n",
    "    \"\"\"Given the soup of the details page, return the card text for a trainer card.\"\"\"\n",
    "    items=[]\n",
    "    for i,tag in enumerate(card_text_table):\n",
    "        if not ('display:none' in str(tag)):\n",
    "            if tag.name == 'tr':\n",
    "                for td in tag.findChildren('table'):\n",
    "                    ctd=td.find('td')\n",
    "                    if not ctd:\n",
    "                        continue\n",
    "                    current_item = {'type':'info', 'description':ctd.text.strip()}\n",
    "                    items.append(current_item)\n",
    "    return items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<a href=\"/wiki/Trainer_card_(TCG)\" title=\"Trainer card (TCG)\"><span style=\"color:#000;\">Trainer</span></a>\n",
      "[{'type': 'info', 'description': \"Draw 2 cards. During this turn, your Pokémon's attacks do 20 more damage to your opponent's Active Pokémon (before applying Weakness and Resistance).\"}, {'type': 'info', 'description': 'You may play only 1 Supporter card during your turn (before your attack).'}]\n"
     ]
    }
   ],
   "source": [
    "# example\n",
    "# Prof. Kukui is card 12 in our list\n",
    "prof_kukui = requests.get(url_for(cards['Professor Kukui'].get('details_page_ref','notexist')))\n",
    "soup = BeautifulSoup(prof_kukui.content,'html.parser')\n",
    "\n",
    "\n",
    "isTrainerCard = soup.find(href='/wiki/Trainer_card_(TCG)')\n",
    "print(isTrainerCard) # This is non if not a trainer card\n",
    "\n",
    "if isTrainerCard:\n",
    "    trainer_card_text_table = soup.find(id='Card_text').findNext()\n",
    "    print(extract_trainer_card_text(trainer_card_text_table))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap those functions for each card type up into something more convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def extract_card_text(soup):\n",
    "    \"\"\"Given the soup for a details page, return any available card text\"\"\"\n",
    "    card_text_title = soup.find(id='Card_text')\n",
    "    if card_text_title is not None:\n",
    "        if isTrainerCard(soup):\n",
    "            card_text = extract_trainer_card_text(card_text_title.findNext())\n",
    "        else: \n",
    "            card_text = extract_pokemon_card_text(card_text_title.findNext())\n",
    "    else:\n",
    "        card_text=None\n",
    "    return card_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Card Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm not quite sure how they work yet, but I noticed a list of card numbers on the details page too. These feel like they may be useful in the future so let's extract those too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def get_card_numbers(soup):\n",
    "    \"\"\"Given the details page soup, extract any available card numbers\"\"\"\n",
    "    en_nums = soup.find_all(text='English card no.')\n",
    "    en_nums = list(set([e.find_next().text.strip() for e in en_nums]))       \n",
    "    jp_nums=soup.find_all(text='Japanese card no.')\n",
    "    jp_nums = list(set([e.find_next().text.strip() for e in jp_nums]))\n",
    "    return {'alt_card_num': { 'jp': jp_nums, 'en': en_nums }}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alt_card_num': {'jp': ['096/SM-P',\n",
       "   '004/018',\n",
       "   '059/060',\n",
       "   '054/059',\n",
       "   '066/060',\n",
       "   '118/131'],\n",
       "  'en': ['128/149', '148/149']}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prof_kukui = requests.get(url_for(cards['Professor Kukui'].get('details_page_ref','notexist')))\n",
    "soup = BeautifulSoup(prof_kukui.content,'html.parser')\n",
    "get_card_numbers(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### wrapped up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def update_card_details(cards):\n",
    "    \"\"\"Procedure: Given a list of cards (output of fetch_card_lists()) augment the list with the relevant details.\"\"\"\n",
    "    for card_name,card in tqdm(cards.items()):\n",
    "\n",
    "        # fetch details page \n",
    "        soup=fetch_page_soup(card.get('details_page_ref','notexist'))\n",
    "        \n",
    "        if soup is not None:\n",
    "            \n",
    "            # extract and add the image urls\n",
    "            card.update({'img_urls': extract_image_urls(soup,card.get('sname'))})\n",
    "            \n",
    "            # extract card text\n",
    "            card.update({'card_text': extract_card_text(soup)})\n",
    "            \n",
    "            # get alternative card numbers\n",
    "            card.update({'alt_card_num': get_card_numbers(soup)})\n",
    "\n",
    "        else:\n",
    "            logging.warning(f\"Couldn't fetch page: {bulba_url_for(card.get('details_page_ref'))}\")\n",
    "    \n",
    "    return None\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching card images and storing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "def fetch_image(img_url):\n",
    "    \"\"\"Given a url produced by extract_image_urls() fetch the image content (bytes) and the filename.\"\"\"\n",
    "    r = requests.get(f'https:{img_url}')\n",
    "    if r.ok:\n",
    "        data = r.content\n",
    "        fname = img_url.split('/')[-1]\n",
    "    else:\n",
    "        logging.warning(f'Failed to fetch image {img_url}')\n",
    "        data=None;fname=None\n",
    "    return (data, fname)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def save_image(image_data, fname): \n",
    "    if 's3' in fname[:2]:\n",
    "        # push to an s3 bucket\n",
    "        s3 = boto3.resource('s3')\n",
    "        bucket=s3.Bucket(fname[3:].split('/')[0])\n",
    "        key='/'.join(fname[3:].split('/')[1:])\n",
    "        ret = bucket.put_object(Key=key, Body=BytesIO(image_data))\n",
    "        if not ret:\n",
    "            logging.error(f\"Failed to upload {fname} to S3.\")\n",
    "        return ret\n",
    "    else:\n",
    "        # assume local file\n",
    "        p=Path(os.path.dirname(fname))\n",
    "        p.mkdir(parents=True, exist_ok=True)\n",
    "        with open(fname,'wb') as f:\n",
    "            f.write(image_data)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export \n",
    "PROJ_HOME = f\"{os.environ['HOME']}/projects/pokemon/\"\n",
    "def fetch_images_for_cards(cards):\n",
    "    for card_name, card in tqdm(cards.items()):\n",
    "        for url in card.get('img_urls'):\n",
    "            image_data, fname = fetch_image(url)\n",
    "            # folder structure: bucket / card_images / original / <class> / <filename>\n",
    "            #location = f\"s3:{BUCKET_NAME}/card_images/original/{card.get('name')}\"\n",
    "            lfname = f\"{PROJ_HOME}/original/{card.get('sname')}/{fname}\"\n",
    "            save_image(image_data, lfname)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import PIL\n",
    "def fetch_card_img_s3(name,bucket=os.environ.get('POKEDEXR_S3')):\n",
    "    s3 = boto3.resource('s3')\n",
    "    x=s3.Object(bucket, f'card_images/original/{name}').get()\n",
    "    src_img = PIL.imread(BytesIO(x['Body'].read()),0)\n",
    "    return src_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 01_scraper.ipynb.\n",
      "Converted 02_augmentation.ipynb.\n",
      "Converted Evaluation.ipynb.\n",
      "Converted ExampleBlog.ipynb.\n",
      "Converted Training.ipynb.\n",
      "Converted WebService.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "from nbdev.export import notebook2script\n",
    "notebook2script()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
